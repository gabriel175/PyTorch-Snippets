{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Snippets\n",
    "This repository will store useful and repetative code snippets for PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable as var\n",
    "from torch import FloatTensor as ft\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.autograd as autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Splitting\n",
    "(split a Dataset object into 3 sub-datasets: train, val and test. adapted from [here](https://github.com/QuantScientist/Deep-Learning-Boot-Camp/blob/master/day02-PyTORCH-and-PyCUDA/PyTorch/21-PyTorch-CIFAR-10-Custom-data-loader-from-scratch.ipynb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullTrainingDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, full_ds, offset, length):\n",
    "        self.full_ds = full_ds\n",
    "        self.offset = offset\n",
    "        self.length = length\n",
    "        assert len(full_ds) >= offset + length, Exception(\"Parent Dataset not long enough\")\n",
    "        super(FullTrainingDataset, self).__init__()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.full_ds[i + self.offset]\n",
    "\n",
    "\n",
    "def trainTestSplit(dataset, val_share=0.1, test_share=0.15):\n",
    "    val_offset = int(len(dataset) * (1 - val_share - test_share))\n",
    "    test_offset = int(len(dataset) * (1 - test_share))\n",
    "    train_len = val_offset\n",
    "    test_len = len(dataset) - test_offset\n",
    "    val_len = len(dataset) - val_offset - test_len\n",
    "    assert train_len + test_len + val_len == len(dataset)\n",
    "    return FullTrainingDataset(dataset, 0, val_offset), \\\n",
    "           FullTrainingDataset(dataset, val_offset, val_len), \\\n",
    "           FullTrainingDataset(dataset, test_offset, test_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Loading/Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, filename='models/checkpoint.pth.tar'):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, 'models/model_best.pth.tar')\n",
    "        with open('models/model_best_summary.txt', 'wb') as f:\n",
    "            f.write(state['summary'])\n",
    "    print('==> saved {}'.format('(*)' if is_best else ''))\n",
    "\n",
    "\n",
    "def load_checkpoint(model, optimizer, path):\n",
    "    if not path:\n",
    "        print(\"==> creating a new model\")\n",
    "        return 0, float(\"inf\")\n",
    "    if path == '-1':\n",
    "        print(\"==> loading best model\")\n",
    "        path = 'models/model_best.pth.tar'\n",
    "    if os.path.isfile(path):\n",
    "        checkpoint = torch.load(path)\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        best_loss = checkpoint['best_loss']\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        print(\"==> loaded checkpoint '{}' (epoch {})\"\n",
    "              .format(path, checkpoint['epoch']))\n",
    "        return start_epoch, best_loss\n",
    "    else:\n",
    "        print(\"==> no checkpoint found at '{}'\".format(path))\n",
    "\n",
    "# make checkpoint\n",
    "is_best = val_loss < best_loss\n",
    "best_loss = min(best_loss, val_loss)\n",
    "save_checkpoint({\n",
    "    'epoch': epoch + 1,\n",
    "    'state_dict': model.state_dict(),\n",
    "    'best_loss': best_loss,\n",
    "    'optimizer': optimizer.state_dict(),\n",
    "    'summary': str(model)\n",
    "}, is_best)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
