{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Snippets\n",
    "This repository will store useful and repetative code snippets for PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable as var\n",
    "from torch import FloatTensor as ft\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.autograd as autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Splitting\n",
    "(split a Dataset object into 3 sub-datasets: train, val and test. adapted from [here](https://github.com/QuantScientist/Deep-Learning-Boot-Camp/blob/master/day02-PyTORCH-and-PyCUDA/PyTorch/21-PyTorch-CIFAR-10-Custom-data-loader-from-scratch.ipynb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FullTrainingDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, full_ds, offset, length):\n",
    "        self.full_ds = full_ds\n",
    "        self.offset = offset\n",
    "        self.length = length\n",
    "        assert len(full_ds) >= offset + length, Exception(\"Parent Dataset not long enough\")\n",
    "        super(FullTrainingDataset, self).__init__()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.full_ds[i + self.offset]\n",
    "\n",
    "\n",
    "def trainTestSplit(dataset, val_share=0.1, test_share=0.15):\n",
    "    val_offset = int(len(dataset) * (1 - val_share - test_share))\n",
    "    test_offset = int(len(dataset) * (1 - test_share))\n",
    "    train_len = val_offset\n",
    "    test_len = len(dataset) - test_offset\n",
    "    val_len = len(dataset) - val_offset - test_len\n",
    "    assert train_len + test_len + val_len == len(dataset)\n",
    "    return FullTrainingDataset(dataset, 0, val_offset), \\\n",
    "           FullTrainingDataset(dataset, val_offset, val_len), \\\n",
    "           FullTrainingDataset(dataset, test_offset, test_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Transformations\n",
    "## General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numbers\n",
    "import torch\n",
    "\n",
    "\n",
    "def pad_tensor(vec, pad, dim):\n",
    "    \"\"\"\n",
    "    args:\n",
    "        vec - tensor to pad\n",
    "        pad - the size to pad to\n",
    "        dim - dimension to pad\n",
    "\n",
    "    return:\n",
    "        a new tensor padded to 'pad' in dimension 'dim'\n",
    "    \"\"\"\n",
    "    return torch.cat([vec, vec.new(pad - vec.size(dim), *vec.size()[1:]).zero_()], dim=dim)\n",
    "\n",
    "\n",
    "class DimZeroPad(object):\n",
    "    \"\"\" Zero pad a tensor along a given dimension \"\"\"\n",
    "\n",
    "    def __init__(self, padding, dim=0):\n",
    "        \"\"\"\n",
    "        padding - sequence length after padding\n",
    "        dim - the dimension to be padded\n",
    "        \"\"\"\n",
    "        assert isinstance(padding, numbers.Number)\n",
    "        self.padding = padding\n",
    "        self.dim = dim\n",
    "\n",
    "    def __call__(self, vec):\n",
    "        assert (vec.dim() - 1) >= self.dim, \"Padded dim doesn't exist!\"\n",
    "        return torch.cat([vec, vec.new(self.padding - vec.size(self.dim), *vec.size()[1:]).zero_()], dim=self.dim)\n",
    "    \n",
    "    \n",
    "class NumpyToTensor(object):\n",
    "    \"\"\" Transform a numpy array to a torch Tensor \"\"\"\n",
    "\n",
    "    def __init__(self, is_float=True):\n",
    "        self.is_float = is_float\n",
    "\n",
    "    def __call__(self, numpy_arr):\n",
    "        x = torch.from_numpy(numpy_arr)\n",
    "        if self.is_float:\n",
    "            return x.float()\n",
    "        return x\n",
    "    \n",
    "\n",
    "class DimZeroPad(object):\n",
    "    \"\"\" Zero pad a tensor along a given dimension \"\"\"\n",
    "\n",
    "    def __init__(self, padding, dim=0):\n",
    "        assert isinstance(padding, numbers.Number)\n",
    "        self.padding = padding\n",
    "        self.dim = dim\n",
    "\n",
    "    def __call__(self, vec):\n",
    "        assert (vec.dim() - 1) >= self.dim, \"Padded dim doesn't exist!\"\n",
    "        return pad_tensor(vec, self.padding, self.dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per-Batch Padding\n",
    "When dealing with variable-length sequences it easy often convinient to zero-pad all sequences to a fixed length, this is speifically true in the case of mini-batch optimization. Padding all examples in the dataset is not efficient, resulting in too large sequences. The following object is a variate of the `collate_fn` passed to the `DataLoader` object. Its function is to pad sequences per-batch, so all examples in the batch are of the same length, but lengths among different batch may vary. This results in more efficient computing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PadCollate:\n",
    "    \"\"\"\n",
    "    a variant of callate_fn that pads according to the longest sequence in\n",
    "    a batch of sequences\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim=0):\n",
    "        \"\"\"\n",
    "        args:\n",
    "            dim - the dimension to be padded (dimension of time in sequences)\n",
    "        \"\"\"\n",
    "        self.dim = dim\n",
    "\n",
    "    def pad_collate(self, batch):\n",
    "        \"\"\"\n",
    "        args:\n",
    "            batch - list of (tensor, label)\n",
    "\n",
    "        reutrn:\n",
    "            xs - a tensor of all examples in 'batch' after padding\n",
    "            ys - a LongTensor of all labels in batch\n",
    "        \"\"\"\n",
    "        # find longest sequence\n",
    "        max_len = max(map(lambda x: x[0].shape[self.dim], batch))\n",
    "        # pad according to max_len\n",
    "        batch = map(lambda (x, y):\n",
    "                    (pad_tensor(x, pad=max_len, dim=self.dim), y), batch)\n",
    "        # stack all\n",
    "        xs = torch.stack(map(lambda x: x[0], batch), dim=0)\n",
    "        ys = torch.LongTensor(map(lambda x: x[1], batch))\n",
    "        return xs, ys\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        return self.pad_collate(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It may be used as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(ds, ..., collate_fn=PadCollate(dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio\n",
    "The following transformations are to be used in a pipline fasion. It assumes that your `Dataset` object returns tuples of (path_to_a_wav, label)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.io.wavfile import read\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "class PathToWav(object):\n",
    "    \"\"\" Transform a path to a wav file into a 1d numpy array \"\"\"\n",
    "\n",
    "    def __call__(self, path):\n",
    "        sr, sig = read(path)\n",
    "        return sig, sr\n",
    "\n",
    "\n",
    "class ProbabilityTransform(object):\n",
    "    \"\"\" apply transform with a probability \"\"\"\n",
    "\n",
    "    def __init__(self, p):\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, x):\n",
    "        if random.random() < self.p:\n",
    "            return self.transform(x)\n",
    "        return x\n",
    "\n",
    "    def transform(self, x):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class RandomWhiteNoise(ProbabilityTransform):\n",
    "    \"\"\" Adds random white noise to signal \"\"\"\n",
    "\n",
    "    def __init__(self, noise_strength=0.005, p=0.0):\n",
    "        self.noise_strength = noise_strength\n",
    "        super(RandomWhiteNoise, self).__init__(p)\n",
    "\n",
    "\n",
    "    def transform(self, (sig, sr)):\n",
    "        wn = np.random.randn(len(sig))\n",
    "        sig_wn = sig + self.noise_strength * wn\n",
    "        return sig_wn, sr\n",
    "\n",
    "\n",
    "class RandomShift(ProbabilityTransform):\n",
    "    \"\"\" Adds a random shift to the signal \"\"\"\n",
    "\n",
    "    def __init__(self, shift_percent=0.1, p=1.0):\n",
    "        self.shift_percent = shift_percent\n",
    "        super(RandomShift, self).__init__(p)\n",
    "\n",
    "    def transform(self, (sig, sr)):\n",
    "        direction = random.choice([-1, 1])\n",
    "        shift = int(direction * len(sig) * self.shift_percent)\n",
    "        return np.roll(sig, shift), sr\n",
    "\n",
    "\n",
    "class WavToSpectrogram(object):\n",
    "    \"\"\" Transform a 1d numpy array to a 2d spectrogram \"\"\"\n",
    "\n",
    "    def __call__(self, (sig, sr)):\n",
    "        frequencies, times, spectrogram = signal.spectrogram(sig, sr)\n",
    "        return np.transpose(spectrogram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It may be used as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([PathToWav(),\n",
    "                       RandomWhiteNoise(100.0),\n",
    "                       RandomShift(0.3),\n",
    "                       WavToSpectrogram(),\n",
    "                       NumpyToTensor(),\n",
    "                       DimZeroPad(padding=SEQ_PADDING, dim=0)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Loading/Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, filename='models/checkpoint.pth.tar'):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, 'models/model_best.pth.tar')\n",
    "        with open('models/model_best_summary.txt', 'wb') as f:\n",
    "            f.write(state['summary'])\n",
    "    print('==> saved {}'.format('(*)' if is_best else ''))\n",
    "\n",
    "\n",
    "def load_checkpoint(model, optimizer, path):\n",
    "    if not path:\n",
    "        print(\"==> creating a new model\")\n",
    "        return 0, float(\"inf\")\n",
    "    if path == '-1':\n",
    "        print(\"==> loading best model\")\n",
    "        path = 'models/model_best.pth.tar'\n",
    "    if os.path.isfile(path):\n",
    "        checkpoint = torch.load(path)\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        best_loss = checkpoint['best_loss']\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        print(\"==> loaded checkpoint '{}' (epoch {})\"\n",
    "              .format(path, checkpoint['epoch']))\n",
    "        return start_epoch, best_loss\n",
    "    else:\n",
    "        print(\"==> no checkpoint found at '{}'\".format(path))\n",
    "\n",
    "# make checkpoint\n",
    "is_best = val_loss < best_loss\n",
    "best_loss = min(best_loss, val_loss)\n",
    "save_checkpoint({\n",
    "    'epoch': epoch + 1,\n",
    "    'state_dict': model.state_dict(),\n",
    "    'best_loss': best_loss,\n",
    "    'optimizer': optimizer.state_dict(),\n",
    "    'summary': str(model)\n",
    "}, is_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN\n",
    "## PackedSequence packing & unpacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sort_and_pack(tensor, lengths):\n",
    "    seq_lengths = lengths\n",
    "    sorted_len, sorted_idx = seq_lengths.sort(0, descending=True)\n",
    "    index_sorted_idx = sorted_idx.view(-1, 1, 1).expand_as(tensor)\n",
    "    sorted_inputs = tensor.gather(0, index_sorted_idx.long())\n",
    "    packed_seq = torch.nn.utils.rnn.pack_padded_sequence(sorted_inputs, sorted_len.cpu().data.numpy(), batch_first=True)\n",
    "    return packed_seq, sorted_idx\n",
    "\n",
    "\n",
    "def unpack_and_unsort(packed, sorted_idx):\n",
    "    unpacked, unpacked_len = torch.nn.utils.rnn.pad_packed_sequence(packed, batch_first=True)\n",
    "    # unsort the output\n",
    "    _, original_idx = sorted_idx.sort(0, descending=False)\n",
    "    unsorted_idx = original_idx.view(-1, 1, 1).expand_as(unpacked)\n",
    "    output = unpacked.gather(0, unsorted_idx.long())\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modules\n",
    "Below are various modules that may save you some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    a block of conv + bn + activation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channel, out_channel, kernel=2, stride=1, padding=0, conv_type='2d', activation='relu', batch_norm=True):\n",
    "        \"\"\"\n",
    "        in_channel, out_channel, kernel=2, stride=1, padding=0 - the same parameters as in pytorch conv1d/conv2d\n",
    "        conv_type - '1d' / '2d', determines the type of convolution\n",
    "        activation - the type of activation to use after the convolution\n",
    "        batch_norm - flag that determines if batch-norm should be used\n",
    "        \"\"\"\n",
    "        super(ConvBlock, self).__init__()\n",
    "        if activation == 'tanh':\n",
    "            activation = nn.Tanh()\n",
    "        elif activation == 'hard_tanh':\n",
    "            activation = nn.Hardtanh(0, 20, inplace=True)\n",
    "        elif activation == 'sigmoid':\n",
    "            activation = nn.Sigmoid()\n",
    "        else:\n",
    "            activation = nn.ReLU()\n",
    "\n",
    "        if conv_type == '2d':\n",
    "            self.conv = nn.Sequential(nn.Conv2d(in_channel, out_channel, kernel, stride, padding),\n",
    "                                      nn.BatchNorm2d(out_channel),\n",
    "                                      activation)\n",
    "        else:\n",
    "            self.conv = nn.Sequential(nn.Conv1d(in_channel, out_channel, kernel, stride, padding),\n",
    "                                      nn.BatchNorm1d(out_channel),\n",
    "                                      activation)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "    \n",
    "    \n",
    "class CollapseConvSeq(nn.Module):\n",
    "    \"\"\"\n",
    "    gets an output of a convolution (batch_size x out_channel x seq_len x feat)\n",
    "    and outputs it to a (batch_size x seq_len x (feat * out_channel))\n",
    "    \n",
    "    it preserves the length of the sequences, just increases the amount of features\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(2, 3).contiguous()\n",
    "        sizes = x.size()\n",
    "        x = x.view(sizes[0], sizes[1] * sizes[2], sizes[3])\n",
    "        return x.transpose(1, 2).contiguous()\n",
    "\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    \"\"\"\n",
    "    flatten to 1d vector\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size(0), -1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
